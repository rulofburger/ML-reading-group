---
title: Household income model
author: ' '
date: '`r Sys.Date()`'
output:
  html_document:
    self contained: no
    number_sections: yes
    theme: yeti
subtitle: A brief demonstration of some machine learning modelling issues using NIDS wave 1 data
---

```{r setup, echo = FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(cache=FALSE)

# rm(list=ls()) 
# graphics.off() 

library(stargazer)
library(locfit)
library(knitr)
library(caret)
library(rpart)

#library(readr)
#library(dplyr)
#library(dygraphs)
#library(xts)
#library(RColorBrewer)

maxvars = 10
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

series.estimation = function(rooms.n,members.n,train) {
  members.series.varlist = paste("cos((members)*", 1:members.n,"*pi/15)",sep="")
  rooms.series.varlist = paste("cos((rooms)*", 1:rooms.n,"*pi/15)",sep="")
  fmla=as.formula(paste("income ~ (",paste(members.series.varlist, collapse= "+"),")*(",paste(rooms.series.varlist, collapse= "+"),")"))
  lm(fmla,subset=train)
}

series.estimation.fmla = function(rooms.n,members.n) {
  members.series.varlist = paste("cos((members)*", 1:members.n,"*3.141593/15)",sep="")
  rooms.series.varlist = paste("cos((rooms)*", 1:rooms.n,"*3.141593/15)",sep="")
  fmla=as.formula(paste("income ~ (",paste(members.series.varlist, collapse= "+"),")*(",paste(rooms.series.varlist, collapse= "+"),")"))
}

load("NIDS.hhdata.small.rda")
sample = hhdata.small$members <= 15 & hhdata.small$rooms <= 15
hhdata.small = hhdata.small[sample,]
for (i in 2:3) {hhdata.small[which(hhdata.small[,i]<0),i]=NA}
hhdata.small$income=as.numeric(hhdata.small$income)
hhdata.small$members=as.integer(hhdata.small$members)
hhdata.small$rooms=as.integer(hhdata.small$rooms)
lm.fit = lm(income~.,data=hhdata.small)
sample.omit = lm.fit$na.action
hhdata.small = hhdata.small[-sample.omit,]
attach(hhdata.small)

```

# Introduction

This document explores the use of a machine learning approach to modelling household income. It attempts to explain the benefit of a flexible functional form, the problem of overfitting on sample data and the need for out-of-sample goodness-of-fit measures. 


# Data and research question

Our objective is to fit as accurately as possible log per capita household income using variables that are easier to measure and verify than household income itself. This research question can be motivated either by the public policy interest in targeting poor households for government transfers using household attributes that are easy to verify, or as a statistical question of whether poor households can be identified with survey questions that are simpler and cheaper to collect than than household income.

We use the NIDS wave 1 data, and for now we restrict our attention to two explanatory variables: the number of houshold members and the number of rooms in the dwelling. Using only two conditioning variables allows us to represent the predicted values on a graph. In order to avoid very low density combinations of the covariate space, we restrict the sample to households with fewer than 15 members and rooms. This leaves us with a dataset containig three variables and 5,805 observations. The density heat map below demonstrates that the majority of sampled households have fewer than 7 rooms and members. 

![Joint density of dwelling rooms and household members](Graphs\xdensity.png)


The analysis starts with biplots of household income against members and rooms. Households that consists of more members tend to have lower average per capita incomes, although this negative relationship appears to grow weaker with more members. The relationship between number of rooms and income is non-linear: an increase in the number of rooms is intially (between 1 to 3 rooms) associated with decreasing average income, but this relationship turns positive at more rooms (between 3 to 10) and then negative again (between 10 and 15 rooms). It is not clear whether this nonlinearity is reflective of the true underlying relationship, or merely reflecting sensitivity to sampling variation in low density areas of the distribution of rooms.

![Bivariate plots: income, members and rooms](Graphs\biplots.png)


# Modeling income

## Linear regressions

The simplest way to model household income as a function of both members and rooms is to use a multivariate regression. The table below reports the estimates from an OLS regression. The table reveals that household income tends to increase by 17% for each additional room and to decrease by 23% for each additional member. Obviously this relationship denotes statistical associations rather than causality. The R-squared for this regression is 0.262 and RMSE is 1.012.

We also show the heat maps of the predicted income for this regression, where blue areas represent high incomes and red areas denote low incomes.

![Predicted income, OLS regression](Graphs\ols.plot1.png)


```{r, results='asis', echo = F, warning=FALSE}
lm.fit1 = lm(income~rooms+members,data=hhdata.small)
#sqrt(mean((income-predict(lm.fit1,hhdata.small))^2))

stargazer(lm.fit1, title="Regression Results",header=FALSE, type='html', align=TRUE, 
          covariate.labels=c("Number of rooms","Number of household members"),
          dep.var.labels=c("Log per capita household income"), omit.stat=c("LL","ser","f"), no.space=TRUE)

```


The linear specification estimated above assumes that log income is a linear additive function of the number of household members and rooms. This may be a convenient approximation, but there is no reason to think that this restrictive functional form is an accurate depiction of the underlying joint distribution between rooms, members and income. One implication of this assumption is that the effect of "adding" a household member on income can always be offset by adding a specific number of rooms, and that this number is invariant to the number of members that are added or to initial number of rooms or members. The regression estimates indicate that, in our example, adding 1.32 rooms will always offset the effect of an additional household member. So a 2 member household with 3 rooms will be  about as well off as a 5 member household with 7 rooms or a 11 member household with 15 rooms. (Notice that these combinations are traced out by the white area of heat map.) Of course, this pattern is completely driven by our heroic functional form assumption, rather than by the characteristics in the data. Will this relationship still hold if we relax the linear additivity assumption? In order to answer this question, we need a more flexible way of modelling the conditional expectation of income. 

## Nonparametric estimators

A more flexible estimator of the conditional expecation of income is provided by nonparametric methods. The simplest such an approach is the K nearest neighbour (KNN) regression technique. This method finds the expected income at various points on the members-rooms grid by taking the sample average of income for the K nearest observations to each grid point. Choosing a small number of neighbours ensures that our local estimate will have a low bias, but will make the estimates more sensitive to sampling variation (i.e. unstable in repeated samples). Larger values of K will produce a smoother response surface that is less sensitive to sampling variation, but this comes at the cost of inducing some bias into the local income estimates (since it now uses information that is further away from the point of interest). For a given value of K, the data will tend to be more biased and more variable in low density areas of the conditioning variables.

The figure below plots the KNN estimates as heat maps for different values of K. We see that when using K=1 expected income changes suddenly between adjacent rooms-members values - especially in low density areas - which is probably a symptom of sampling variability rather than a reflection of the true data generating process. As K increases and each estimate is obtained from more observations, changes in expected income occur more gradually with changing values of rooms and members, but we run the risk of biasing local income estimates.

![KNN estimates](Graphs\multi.knn.png)

Although it is convenient and simple, the KNN method unfortunately produces a discontinuous response surface since small changes in the grid results in a discontinuous change in the observations that are included in the averaging. A superior approach is offered by a local linear regression, which weights observations around the evaluation point so that observations further away from the point of interest carry less weight. If this weighting function, or kernel, is smooth then the resulting response surface will also be smooth.

The figure below compares the local linear regression estimates for different bandwidth values (which determine how gradually the weight attached to each observation converges on zero as we move away from the evaluation point). The estimates are smoother, and we again observe the effect of using more observations (by choosing a larger bandwidth) in terms of producing estimates that are less responsive to small changes in the conditioning variables. In fact, at a bandwith of 15 the local linear regression starts to resemble the globally linear OLS estimates, which may be indicative of the bias that results from oversmoothing. 

![Local linear regression estimates](Graphs\multi.loclin.png)

To demonstrate the effect of the bandwidth on sampling variability, we divide our sample into four equally sized subsamples. Each of the four local linear regressions is estimated on each of these subsamples and the predictions are plotted as heat maps. We observe that in the top panels (where low bandwidths are used) the estimates are more unstable across different subsamples, especially in low density areas of the data. Higher bandwidth values on the other hand (in the lower panels) are more stable across subsamples. 

![Local linear regression estimates, by subsample](Graphs\multi.loclin.16.png)

# Parameter tuning

Reliable out-of-sample prediction requires a model that provides predictions with a low bias and a low variance. The choice of bandwidth - and model complexity more generally - induces a trade-off in this regard: low bandwidth values produce low bias-high variance estimates, whereas high bandwidth values reduce the variance at the cost of increasing the bias. We would like to choose a bandwidth that is small enough to ensure low bias but large enough to avoid substantial sampling variability. Unfortunately, most commonly used in-sample measures of goodness-of-fit, like the R-squared or root mean squared error (RMSE), do not reflect sampling variability and will therefore always recommend using a more complex model that will tend to overfit the sample data.

Choosing an optimal bandwidth requires either using some goodness-of-fit measure that penalises model complexity, like the adjusted R-squared, the Akaike information criteria (AIC) or the Bayesian information criteria (BIC) - or measuring the goodness-of-fit in a sample not also used to fit the model. This can be achieved by splitting the data into two subsamples. The model can then be estimated on one sample (the training sample) and the goodness-of-fit calculated on the second sample (the testing sample). A model that is too complex will overfit the sampling variation in the training sample, but perform poorly in the testing sample. Models of different complexity can then be estimated on the training sample, and their predictive ability compared in the testing sample. Typically, we expect to see the patterns in the graph below (taken from Hastie, Tibshirani and Friedman, 2008). The process of choosing the complexity parameter by comparing out-of-sample predictive ability is known as (hyper)parameter tuning. 

![](Graphs\Figure 2.11.png)

## Local linear regressions
 
For the local linear regressions, the the tuning step entails estimating this model with different bandwidth values and then using out-of-sample evaluation to identify the bandiwidth value that produces the most reliable predictions. The graph below plots the RMSE for the testing and training samples for local linear regressions with various bandwidth values. Instead of splitting the data into one training and one sample, we use 5-fold cross-validation. (This uses the same principle, but produces more accurate estimates of the out-of-sample fit.) Bandwidth values between 1.1 and 6 were explored: the bandwith parameter was increased by increments of 0.1 between 1.1 and 3, and by increments of 1 between values of 3 and 6.  

The training sample RMSE is always lower than test sample RMSE, due to the overoptimism that comes from overfitting a model on sample data. This gap is particularly large for more complex models with lower bandwidths, since more flexible specifications are more vulnerable to overfitting. The testing sample RMSE is U-shaped, and reaches a minimum at a bandwidth of 2.2, so this is the tuned parameter value that we should use to estimate our final model. (Some simulation studies have suggested that we may want to err on the side of choosing less complex models than suggested by parameter tuning.)

![Income model testing and training error as a function of bandwidth](Graphs\Tradeoff.png)

## Linear regressions with polynomials

Local linear regressions provide an intuitive way of flexibly fitting income on rooms and members, but we could also allow more flexibility in our OLS estimator by including non-linear transformations and interaction variables in our regression. Below, we report the regression output for the linear specification (used above) as well as a model that is quadratic in both members and rooms and in which the linear and quadratic terms are interacted. 

```{r, results='asis', echo = F, warning=FALSE}
lm.fit1 = lm(income~rooms+members,data=hhdata.small)
#summary(lm.fit1)
ols1.hat = lm.fit1$fitted.values
lm.fit2 = lm(income~rooms*members+I(rooms^2)*I(members^2),data=hhdata.small)
#summary(lm.fit2)
ols2.hat = lm.fit2$fitted.values
lm.fit3 = lm(income~rooms*members+I(rooms^2)*I(members^2)+poly(rooms,maxvars)*poly(members,maxvars),data=hhdata.small)
ols3.hat = lm.fit3$fitted.values
stargazer(lm.fit1, lm.fit2, title="Regression Results",header=FALSE, type='html', align=TRUE, 
covariate.labels=c("Rooms","Members","Rooms squared","Members squared","Rooms x Members","Rooms squared x Members squared"),
 dep.var.labels=c("Log per capita household income"), omit.stat=c("LL","ser","f"), no.space=TRUE)
```

We also show the heat maps of the predicted income for the two regression models above, as well as two additional specifications in which each predictor is included as a polynomial (of orders 5 and 10 respectively) and  interacted exhuastively. As we move from a linear to a quadratic specification, the R-squared increases from 0.262 to 0.290, although this may be symptomatic of either improved fitting of the population variation of overfitting of the sampling variation. The predicted value graph indicates that the positive association between household income and dwelling rooms is stronger for those with few household members; this relationship was ruled out by the linear additive specification.

The fifth order polynomial specification reveals an even more complicated pattern, and predicted incomes for households with 15 members and 15 rooms are outside of the observable range of income values. This is a common problem when using high order polynomials that are known to be sensitive to small tweaks in the coefficients. But even where the predicted values are within the observable range, we see some surprising patterns. For example, households with the maximum number of rooms are  rich if they have either 1 member, 4-6 members, or 12 household members;  middle income if they 2-3 members; and poor if they have 8-10 members or 13 members. This seems like the sort of pattern that is likely a symptom of overfitting on sample data, rather than being reflective of an unstable data generating process. This sampling variability is often the result of allowing highly flexible specifications, and something we will need to consider carefully when applying ML techniques.

The tenth order polynomial specification produces an even more jagged response surface with large areas of the conditioning set associated with predicted income values outside of the observable income range.


![Linear regression with polynomials expansions and interactions](Graphs\multi.poli.png)



Econometricians will recognise the tradeoff between bias and sampling variability that arises in choosing polynomial expansions and interaction effects: moving from a linear to a quadratic specification and including an interaction makes sense if it substantially improves the model fit, whereas a polynomial of order 10 seems "much too flexible" for the kind of smooth trends we expect to find. The final decision regarding the specification is often made in a haphazardous way, starting with the simplest specification and then checking whether specific interaction effects or non-linear terms improve model fit. But the arbitrariness of this approach makes it vulnerable to misspecification, either because we do not explore all possible specifications or, more sinisterly, because we selectively present the results from specifications that support our preferred hypothesis. 

The statistical learning approach to choosing the correct orders for the polynomial expansions of dwelling rooms and household members would be to remove the modeller's discretion from this decision. We can view the model complexity as now depending on two parameters (the polynomial orders) and proceed to estimate the out-of-sample fit for various combinations of these parameters. We therefore estimate 100 models in which we combine polynomials of the number of dwelling rooms of orders 1 to 10 with polynomials of number of household members of orders 1 to 10. Each of the considered specifications is then estimated on the training sample and the estimates used to obtain predicted values for the testing sample. The root mean squared error (RMSE) is then calculated by comparing the observed and predicted household incomes in the tesing sample. The table below compares the RMSE for the different specifications. The out-of-sample model fit is optimised by a specification with a 4th order polynomial in rooms and a 3rd order polynomial in members.

```{r, results='asis', echo = F, warning=FALSE}

set.seed(825)
fitControl <- trainControl(method = "repeatedcv", number = 5,  repeats = 5)

RMSE=matrix(,nrow=maxvars,ncol=maxvars)
for (rooms.i in (1:maxvars)) {
  for (members.i in (1:maxvars)) {
    j <- bquote(income~poly(rooms,.(rooms.i))*poly(members,.(members.i)))
    LinearRegressor <- train(as.formula(j), data=hhdata.small, method = "lm", trControl = fitControl)
    RMSE[rooms.i,members.i] <- LinearRegressor$results$RMSE
  }
}

dimnames(RMSE) = list(rooms=1:maxvars, members=1:maxvars)
kable(RMSE,row.names=T,col.names=1:maxvars,digits=4,caption="RMSE")
paste("Minimum RMSE is achieved at", which(RMSE==min(RMSE),arr.ind=TRUE)[1], "rooms and", which(RMSE==min(RMSE),arr.ind=TRUE)[2], "members.")

```

## Linear regressions with Fourier transforms

Although polynomial functions offer a useful way of extending the flexibility of linear models, they are known to produce predicted values that are highly sensitive to small adjustments in coefficient values. Tweaking the fit in high density data areas can cause highly variable and implausible predictions in low density areas. Other basis functions, like a series of Fourier transforms, are often preferred for this reason. We estimate a linear regression with Fourier transforms of orders 1, 2, 4 and 6 for each of the predictors and plot the predicted incomes below. 

Again, we observe the familiar pattern of an overly smooth response surface for models of low complexity, whereas models of high complexity produce an implausibly jagged surface with several local modes. The paramters that determine the model complexity - in this case the order for the Fourier transforms - can be tuned by comparing the out-of-sample performance of various models. The table below reports the RMSE values for Fourier transforms of orders 1 to 10 for rooms (rows) and members (columns).


![Fourier series estimates](Graphs\multi.series.png)


```{r, results='asis', echo = F, echo=FALSE,warning=FALSE}
set.seed(825)
fitControl <- trainControl(method = "repeatedcv", number = 5,  repeats = 5)

RMSE=matrix(,nrow=maxvars,ncol=maxvars)
for (rooms.i in (1:maxvars)) {
  for (members.i in (1:maxvars)) {
    LinearRegressor <- train(series.estimation.fmla(rooms.i,members.i), data=hhdata.small, method = "lm", trControl = fitControl)
    RMSE[rooms.i,members.i] <- LinearRegressor$results$RMSE
  }
}

dimnames(RMSE) = list(rooms=1:maxvars, members=1:maxvars)
kable(RMSE,row.names=T,col.names=1:maxvars,digits=4,caption="RMSE")
paste("Minimum RMSE is achieved at", which(RMSE==min(RMSE),arr.ind=TRUE)[1], "rooms and", which(RMSE==min(RMSE),arr.ind=TRUE)[2], "members.")

```

We see that out-of-sample prediction is most reliable when rooms is included as a Fourier transform of order 2 and members as a Fourier transform of order 7. We can use these tuned parameter values to estimate the final model on the entire sample.

## Regression trees

We could also model income using a regression tree. The rpart function in R requires specifying a complexity parameter (cp) which represents the smallest improvement in model fit required to enact a split. We estimate regression trees using 4 different cp values and graph the predicted values. 

![Regression trees](Graphs\multi.tree.png)

The regression tree complexity paramter can be tuned in the same way as before. We find that a cp value of 0.0002 produces the lowest test sample RMSE of 0.9906857.

# Comparison of different models

Finally, we compare the income predictions for the tuned versions of all the models considered above: linear regressions with polynomial and Fourer transform basis function expansions, local linear regressions and regression trees. 

![Best models](Graphs\multi.best.png)

Although several attributes of the response surfaces are quite different (e.g. rectangular for regression trees, smooth for regressions) and the income predictions in the low density areas are quite different, all four models produce similar predicted incomes in the high density regions. In fact, a correlation matrix for the predicted values reveal that the pairwise correlation for any two of these models is never less than 0.976, which indicates that the final predictions are quite robust to the choice of estimator. 

