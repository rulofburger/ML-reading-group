---
title: NIDS Income model
author: ' '
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    theme: yeti
  html_notebook:
    number_sections: yes
    theme: yeti
subtitle: Sleep Classification Model
---

```{r setup, echo = FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(cache=FALSE)

# rm(list=ls()) 
# graphics.off() 

library(readr)
library(dplyr)
library(dygraphs)
library(xts)
library(RColorBrewer)

maxvars = 10
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

# 1. Introduction

This document explores the use of regression tree models implemented via R’s caret package. We will use different estimators to model the household income of a sample of South African households using a variety of continuous and categorical predictors. The model performance, running time and relative merits of the different approaches will be compared.


# 2. Data and research question

Our objective is to fit as accurately as possible log per capita household income using a variety of other variables. This research question can be motivated either by the public policy interest in targeting poor households for government transfers using variables that are easier to verify than household income, or by statistical interest in identifying poor households using variables that are simpler to measure with surveys than than household income.

The NIDS wave 1 data contains 6,242 households. We model household income using 4 predictors that are roughly continuous (numbers of household members and dwelling rooms, as well as the household head’s age and years of completed education) and 29 nominal variables (including province and district council, dwelling attributes, household assets, the availability of various utilities and the demographic attributes of the household head). Observations with very uncommon values of these nominal variables are dropped because of problems this causes with cross-validation (you cannot predict the effect of a an attribute that is unobserved in the trainingh data), and these variables are transformed to 191 binary variables using the dummy.data.frame function. The final data set is called hhdata.full.

# 3. Data & descriptive statistics

In order to explain some important concepts in machine learning (ML) we use the NIDS wave 1 data to model expected log per capita household income as a function of two explanatory variables: the number of houshold members and the number of rooms in the dwelling. Using only two conditioning variables allows us to represented the predicted values on a graph. In order to avoid very low density combinations of the covariate space, We restrict the sample to households with fewer than 15 members and rooms. The density heat map below demonstrates that the majority of sampled households have fewer than 7 rooms and members. 

```{r, warning = FALSE}
![Joint density of rooms and members.](/xdensity.png)
```

The analysis starts with biplots of household income against members and rooms. Households that consists of more members tend to have lower average per capita incomes, although this negative relationship appears to grow weaker with more members. The relationship between number of rooms and income is non-linear: an increase in the number of rooms is intially (from 1 to 3 rooms) associated with decreasing average income, but this relationship turns positive at more rooms (from 3 to 10) and then negative again (between 10 and 15 rooms). It is not clear whether this nonlinearity is reflective of the true underlying relationship, or merely reflecting sensitivity to sampling variation in low density areas of the distribution of rooms.

```{r, warning = FALSE}

```


# 2. Modeling income

### Linear regressions

The simplest way of expressing household income as a function of both members and rooms is to use a multivariate regression. The table below reports the estimates from an OLS regression. The table reveals that household income tends to increase by 17% for each additional room and to decrease by 23% for each additional member. Obviously this relationship denotes statistical associations rather than causality. The RMSE for this regression is 1.31.

We also show the heat maps of the predicted income for this regression, where blue areas represent high incomes and red areas denote low incomes.

```{r, results='asis',warning=FALSE}
lm.fit1 = lm(income~rooms+members,data=hhdata)
#summary(lm.fit1)
#sqrt(mean((income-predict(lm.fit1,hhdata))^2))

stargazer(lm.fit1, title="Regression Results",header=FALSE, type='html', align=TRUE, 
covariate.labels=c("Number of rooms","Number of household members"),
 dep.var.labels=c("Log per capita household income"), omit.stat=c("LL","ser","f"), no.space=TRUE)

my.colors = colorRampPalette(c("#a50026", "red","orange","yellow","white","#4575b4", "#313695","black"))
grid.graph=data.frame(members=rep(seq(1,max(hhdata$members),0.1),each=((max(hhdata$rooms)-1)*10+1)),rooms=rep(seq(1,max(hhdata$rooms),0.1),((max(hhdata$members)-1)*10+1)))
h <-ggplot(grid.graph,aes(members, rooms))

ols1.hat.grid <- predict(lm.fit1, grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
h + geom_point(aes(color = ols1.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("OLS")

```

The linear specification estimated above assumes that income is a linear additive function of the numbers of household members and rooms. This may be a convenient approximation, but there is no reason to think that this restrictive functional form is an accurate depiction of the underlying distribution between rooms, members and income. One implication of this assumption is that the effect of "adding" a household member on income can always be offset by adding a specific number of rooms, and that this number is invariant to the number of members that are added or to the household's initial income. The regression estimates indicate that, in our example, adding 1.35 rooms will always offset the effect of an additional household member. So a 1 member household with 3 rooms will be  about as well off as a 4 member household with 7 rooms or a 10 member household with 15 rooms. (Notice that these combinations trace out the white area of heat map.) Of course, this pattern is completely driven by our heroic functional form assumption, rather than by the characteristics in the data. Will this relationship still hold if we relax the linear additivity assumption? In order to answer this question, we need a more flexible way of modelling the conditional expectation of income. 

### Nonparametric estimators

A more flexible estimator of the conditional expecation of income is provided by nonparametric methods. The simplest such an approach is the K nearest neighbour (KNN) regression technique. This method finds the expected income at various points on the members-rooms grid by taking the sample average of income for the K nearest observations to each point. Choosing a small number of neighbours ensures that our estimate will have a very low bias, but will make the estimates very sensitive to sampling variation (i.e. unstable in repeated samples). Larger values of K will produce smoother and more stable response surfaces, but at the cost of inducing some bias into the local income estimates. For a given value of K, the data will tend to be more biased and more variable in low density areas of the conditioning variables.

The figure below plots the KNN estimates as heat maps for different values of K. We see that when using K=1 expected income changes suddenly between adjacent values rooms-members, especially in low density areas, which is probably a symptom of sampling variability rather than a reflection of the true data generating process. As K increases and each estimate is obtained from more observations, changes in expected income occur more gradually, but we run the risk of biasing local income estimates.

```{r, results='asis', echo=FALSE,warning=FALSE}
grid.graph1=data.frame(members=rep(seq(1,max(hhdata$members),0.025),each=((max(hhdata$rooms)-1)*40+1)),rooms=rep(seq(1,max(hhdata$rooms),0.025),((max(hhdata$members)-1)*40+1)))
h1 <-ggplot(grid.graph1,aes(members, rooms))
grid.graph1 = grid.graph1 + matrix(rnorm(dim.data.frame(grid.graph1)[1]*2,mean = 0, sd = 0.00000001), ncol=2)

knn1.fit=knnreg(hhdata[,2:3],hhdata[,1],test=NULL , k = 100)
knn1.hat.grid = predict(knn1.fit,grid.graph1)
knn2.fit=knnreg(hhdata[,2:3],hhdata[,1],test=NULL , k = 20)
knn2.hat.grid = predict(knn2.fit,grid.graph1)
knn3.fit=knnreg(hhdata[,2:3],hhdata[,1],test=NULL , k = 5)
knn3.hat.grid = predict(knn3.fit,grid.graph1)
knn4.fit=knnreg(hhdata[,2:3],hhdata[,1],test=NULL , k = 1)
knn4.hat.grid = predict(knn4.fit,grid.graph1)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig4 = h1 + geom_jitter(aes(color = knn1.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("100 Nearest Neighbours")
fig3 = h1 + geom_jitter(aes(color = knn2.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("20 Nearest Neighbours")
fig2 = h1 + geom_jitter(aes(color = knn3.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("5 Nearest Neighbours")
fig1 = h1 + geom_jitter(aes(color = knn4.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("1 Nearest Neighbour")

multiplot(fig1, fig3, fig2, fig4, cols=2)


```

Although very simple, the KNN method unfortunately produces a discontinuous response surface since small changes in the grid results in a discontinuous change in the observations that are included in the averaging. A superior approach is offered by a local linear regression, which weights observations around the evaluation point so that points further away carry less weight. If this weighting function, or kernel, is smooth then the resulting response surface will also be smooth.

The figure below compares the local linear regression estimates for different bandwidth values (which determine how gradually the weight attached to each observation converges on zero as we move away from the evaluation point). The estimates are smoother, and we again observe the effect of using more observations (by choosing a larger bandwidth) in terms of producing estimates that are less responsive to small changes in the conditioning variables. In fact, at a bandwith of 15 the local linear regression starts to resemble the globally linear OLS estimates, which may be indicative of the bias that results from oversmoothing. 

```{r, results='asis', echo=FALSE,warning=FALSE}

knn1.fit = locfit(income~members+rooms,alpha=c(0,1.1),kern="epan",deg=1)
knn1.hat.grid = predict(knn1.fit,grid.graph)
knn2.fit = locfit(income~members+rooms,alpha=c(0,3),kern="epan",deg=1)
knn2.hat.grid = predict(knn2.fit,grid.graph)
knn3.fit = locfit(income~members+rooms,alpha=c(0,5),kern="epan",deg=1)
knn3.hat.grid = predict(knn3.fit,grid.graph)
knn4.fit = locfit(income~members+rooms,alpha=c(0,15),kern="epan",deg=1)
knn4.hat.grid = predict(knn4.fit,grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig1 = h + geom_point(aes(color = knn1.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("Bandwidth = 1.1")
fig2 = h + geom_point(aes(color = knn2.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Bandwidth = 3")
fig3 = h + geom_point(aes(color = knn3.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Bandwidth = 5")
fig4 = h + geom_point(aes(color = knn4.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Bandwidth = 15")

multiplot(fig1, fig3, fig2, fig4, cols=2)
```

In order to demonstrate the effect of the bandwidth on sampling variability, we divide our sample into four subsamples. Each of the four local linear regressions is then estimated on each of these subsamples and the predictions are plotted as heat maps. We observe that in the top panels (where low bandwidths are used) the estimates are more unstable across different subsamples, especially in low density areas of the data. Higher bandwidth values on the other hand (in the lower panels) are more stable across subsamples. 

```{r, results='asis', echo=FALSE,warning=FALSE}
set.seed(1234567890)
#sample1 = sample(1:length(hhdata$income),length(hhdata$income)/2,replace=T)
#sample2 = - sample1
#sample3 = sample(1:length(hhdata$income),length(hhdata$income)/2,replace=T)
#sample4 = - sample2
sample1 = sample(c(1:length(hhdata$income)),length(hhdata$income)/4)
sample2 = sample(c(1:length(hhdata$income))[-sample1],length(hhdata$income)/4)
sample3 = sample(c(1:length(hhdata$income))[-c(sample1,sample2)],length(hhdata$income)/4)
sample4 = c(1:length(hhdata$income))[-c(sample1,sample2,sample3)]

knn1.fit1 = locfit(income~members+rooms,alpha=c(0,1.1),kern="epan",deg=1,data=hhdata[sample1,])
knn1.hat.grid1 = predict(knn1.fit1,grid.graph)
knn1.fit2 = locfit(income~members+rooms,alpha=c(0,1.1),kern="epan",deg=1,data=hhdata[sample2,])
knn1.hat.grid2 = predict(knn1.fit2,grid.graph)
knn1.fit3 = locfit(income~members+rooms,alpha=c(0,1.1),kern="epan",deg=1,data=hhdata[sample3,])
knn1.hat.grid3 = predict(knn1.fit3,grid.graph)
knn1.fit4 = locfit(income~members+rooms,alpha=c(0,1.1),kern="epan",deg=1,data=hhdata[sample4,])
knn1.hat.grid4 = predict(knn1.fit4,grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig11 = h + geom_point(aes(color = knn1.hat.grid1)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig12 = h + geom_point(aes(color = knn1.hat.grid2)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig13 = h + geom_point(aes(color = knn1.hat.grid3)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig14 = h + geom_point(aes(color = knn1.hat.grid4)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())

knn2.fit1 = locfit(income~members+rooms,alpha=c(0,3),kern="epan",deg=1,data=hhdata[sample1,])
knn2.hat.grid1 = predict(knn2.fit1,grid.graph)
knn2.fit2 = locfit(income~members+rooms,alpha=c(0,3),kern="epan",deg=1,data=hhdata[sample2,])
knn2.hat.grid2 = predict(knn2.fit2,grid.graph)
knn2.fit3 = locfit(income~members+rooms,alpha=c(0,3),kern="epan",deg=1,data=hhdata[sample3,])
knn2.hat.grid3 = predict(knn2.fit3,grid.graph)
knn2.fit4 = locfit(income~members+rooms,alpha=c(0,3),kern="epan",deg=1,data=hhdata[sample4,])
knn2.hat.grid4 = predict(knn2.fit4,grid.graph)

fig21 = h + geom_point(aes(color = knn2.hat.grid1)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig22 = h + geom_point(aes(color = knn2.hat.grid2)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig23 = h + geom_point(aes(color = knn2.hat.grid3)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig24 = h + geom_point(aes(color = knn2.hat.grid4)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())

knn3.fit1 = locfit(income~members+rooms,alpha=c(0,5),kern="epan",deg=1,data=hhdata[sample1,])
knn3.hat.grid1 = predict(knn3.fit1,grid.graph)
knn3.fit2 = locfit(income~members+rooms,alpha=c(0,5),kern="epan",deg=1,data=hhdata[sample2,])
knn3.hat.grid2 = predict(knn3.fit2,grid.graph)
knn3.fit3 = locfit(income~members+rooms,alpha=c(0,5),kern="epan",deg=1,data=hhdata[sample3,])
knn3.hat.grid3 = predict(knn3.fit3,grid.graph)
knn3.fit4 = locfit(income~members+rooms,alpha=c(0,5),kern="epan",deg=1,data=hhdata[sample4,])
knn3.hat.grid4 = predict(knn3.fit4,grid.graph)

fig31 = h + geom_point(aes(color = knn3.hat.grid1)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig32 = h + geom_point(aes(color = knn3.hat.grid2)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig33 = h + geom_point(aes(color = knn3.hat.grid3)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig34 = h + geom_point(aes(color = knn3.hat.grid4)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())

knn4.fit1 = locfit(income~members+rooms,alpha=c(0,15),kern="epan",deg=1,data=hhdata[sample1,])
knn4.hat.grid1 = predict(knn4.fit1,grid.graph)
knn4.fit2 = locfit(income~members+rooms,alpha=c(0,15),kern="epan",deg=1,data=hhdata[sample2,])
knn4.hat.grid2 = predict(knn4.fit2,grid.graph)
knn4.fit3 = locfit(income~members+rooms,alpha=c(0,15),kern="epan",deg=1,data=hhdata[sample3,])
knn4.hat.grid3 = predict(knn4.fit3,grid.graph)
knn4.fit4 = locfit(income~members+rooms,alpha=c(0,15),kern="epan",deg=1,data=hhdata[sample4,])
knn4.hat.grid4 = predict(knn4.fit4,grid.graph)

fig41 = h + geom_point(aes(color = knn4.hat.grid1)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig42 = h + geom_point(aes(color = knn4.hat.grid2)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig43 = h + geom_point(aes(color = knn4.hat.grid3)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
fig44 = h + geom_point(aes(color = knn4.hat.grid4)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l,guide=F)  +   theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank(),axis.title.y=element_blank(),axis.text.y=element_blank(),axis.ticks.y=element_blank())
multiplot(fig11, fig21,fig31,fig41,fig12,fig22,fig32,fig42, fig13,fig23,fig33,fig43, fig14,fig24,fig34,fig44, cols=4)
```

# 3. Parameter tuning

In order to obtain reliable out-of-sample predictions, we need a model that provides predictions with a low bias and a low variance. The choice of bandwidth - and model complexity more generally - induces a trade-off in this regard: low bandwidth values produce low bias, high variance estimates, whereas high bandwidth values reduce the variance at the cost of increasing the bias. We would like to choose a bandwidth that is small enough to ensure low bias but large enough to avoid substantial sampling variation. Unfortunately, most commonly used in-sample measures of goodness-of-fit, like the R-squared or root mean squared error (RMSE), do not reflect sampling variability and will therefore always recommend using a more flexible model that overfits the sample data. 

Choosing an optimal bandwidth requires either using a goodness-of-fit measure that penalises model complexity, like the adjusted R-squared, the Akaike information criteria (AIC) or the Bayesian information criteria (BIC) - or measuring the goodness-of-fit in a sample that was not also used to fit the model. This can be achieved by splitting the data into training and testing samples, and then calculating goodness-of-fit measures on the test sample for models that were fit on the training sample. Typically, we expect to see patterns as in the graph below (taken from Hastie, Tibshirani and Friedman, 2008). The process of estimating various models and then choosing the model complexity based on test sample performance is known as paramter tuning.


 
### Local linear regressions
 
The tuning step requires estimating models with different complexitiy parameters (e.g. the bandwidth in local linear regressions) and then using out-of-sample evaluation to identify the parameter values that produces the most reliable predictions. The graph below plots the RMSE for the testing and training samples for local linear regressions with various bandwidth values. The training sample RMSE is always lower than for the test sample, due to the overoptimism that comes from overfitting on a sample. This gap is particularly large for more complex models with lower bandwidths, since more flexible specifications are more probe to overfitting. The testing sample RMSE is U-shaped, and reaches a minimum at a bandwidth of 1.7, so this is the tuned parameter value that we should use to estimate our final model.

```{r, echo=FALSE, echo=FALSE,warning=FALSE}

set.seed(825)
sample1 = sample(c(1:length(hhdata$income)),length(hhdata$income)/5)
sample2 = sample(c(1:length(hhdata$income))[-sample1],length(hhdata$income)/5)
sample3 = sample(c(1:length(hhdata$income))[-c(sample1,sample2)],length(hhdata$income)/5)
sample4 = sample(c(1:length(hhdata$income))[-c(sample1,sample2,sample3)],length(hhdata$income)/5)
sample5 = c(1:length(hhdata$income))[-c(sample1,sample2,sample3,sample4)]

cv.eval.test = function(bw) {
  RMSE.test=rep(0,5)
  knn.fit1 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[-sample1,])
  knn.fit2 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[-sample2,])
  knn.fit3 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[-sample3,])
  knn.fit4 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[-sample4,])
  knn.fit5 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[-sample5,])
  RMSE.test[1] = sqrt(mean((income[sample1]-predict(knn.fit1,hhdata[sample1,]))^2))
  RMSE.test[2] = sqrt(mean((income[sample2]-predict(knn.fit2,hhdata[sample2,]))^2))
  RMSE.test[3] = sqrt(mean((income[sample3]-predict(knn.fit3,hhdata[sample3,]))^2))
  RMSE.test[4] = sqrt(mean((income[sample4]-predict(knn.fit4,hhdata[sample4,]))^2))
  RMSE.test[5] = sqrt(mean((income[sample5]-predict(knn.fit5,hhdata[sample5,]))^2))
  mean(RMSE.test)
}

cv.eval.train = function(bw) {
  RMSE.train=rep(0,5)
  knn.fit1 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[sample1,])
  knn.fit2 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[sample2,])
  knn.fit3 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[sample3,])
  knn.fit4 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[sample4,])
  knn.fit5 = locfit(income~members+rooms,alpha=c(0,bw),kern="epan",deg=1,data=hhdata[sample5,])
  RMSE.train[1] = sqrt(mean((income[sample1]-predict(knn.fit1,hhdata[sample1,]))^2))
  RMSE.train[2] = sqrt(mean((income[sample2]-predict(knn.fit2,hhdata[sample2,]))^2))
  RMSE.train[3] = sqrt(mean((income[sample3]-predict(knn.fit3,hhdata[sample3,]))^2))
  RMSE.train[4] = sqrt(mean((income[sample4]-predict(knn.fit4,hhdata[sample4,]))^2))
  RMSE.train[5] = sqrt(mean((income[sample5]-predict(knn.fit5,hhdata[sample5,]))^2))
  mean(RMSE.train)
}

bw = c(seq(from = 1.1, to = 3, by =0.1),4:6)
cv.error.test= rep(0 ,23)
cv.error.train= rep(0 ,23)
for (i in 1:23) {
  cv.error.test[i]=cv.eval.test(bw[i])
  cv.error.train[i]=cv.eval.train(bw[i])
}

plot(c(1,6),c(0.94,1),type="n",xlab="Bandwidth",ylab="RMSE")
lines(bw,cv.error.test,col="red",lwd=2.5)
lines(bw,cv.error.train,col="blue",lwd=2.5)
legend(4,0.96,c("Test sample","Train sample"),lty=c(1,1,1,1),lwd=c(2.5,2.5),col=c("red","blue"))

knn.fit.best = locfit(income~members+rooms,alpha=c(0,1.7),kern="epan",deg=1,data=hhdata)
knn.best.grid = predict(knn.fit.best,grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
best.knn = h + geom_point(aes(color = knn.best.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Local linear")

```

### Linear regressions with polynomials

Local linear regressions provide an intuitive way of flexibly fitting income on rooms and members, but we could also allow more flexibility in our OLS estimator by including non-linear transformations and interaction variables in our regression. Below, we report the regression output for the linear specification (used above) as well as a model that is quadratic in both members and rooms and in which the linear and quadratic terms are interacted. 

```{r, results='asis',warning=FALSE}
lm.fit1 = lm(income~rooms+members,data=hhdata)
#summary(lm.fit1)
ols1.hat = lm.fit1$fitted.values
lm.fit2 = lm(income~rooms*members+I(rooms^2)*I(members^2),data=hhdata)
#summary(lm.fit2)
ols2.hat = lm.fit2$fitted.values
lm.fit3 = lm(income~rooms*members+I(rooms^2)*I(members^2)+poly(rooms,maxvars)*poly(members,maxvars),data=hhdata)
ols3.hat = lm.fit3$fitted.values

stargazer(lm.fit1, lm.fit2, title="Regression Results",header=FALSE, type='html', align=TRUE, 
covariate.labels=c("Rooms","Members","Rooms squared","Members squared","Rooms x Members","Rooms squared x Members squared"),
 dep.var.labels=c("Log per capita household income"), omit.stat=c("LL","ser","f"), no.space=TRUE)
```

We also show the heat maps of the predicted income for the two regression models above, as well as two additional specifications in which each predictor is included as a polynomial (of orders 5 and 10 respectively) and  interacted exhuastively. The benefit of added flexibility is clear as we move from a linear to a quadratic specification. It now seems that the positive association between rooms and income is stronger for those with few household members, which is also what the biplot in the descriptive analysis showed. 

The fifth order polynomial specification reveals an even more complicated pattern, and predicted incomes for households with 15 members and 15 rooms are outside of the observable range of income values. This is a common problem when using high order polynomials that are known to be sensitive to small tweaks in the coefficients. But even where the predicted values are within the observable range, we see some surprising patterns. For example, households with the maximum number of rooms are extremely rich if they have either 1 member, or 5-6 members, but extremely poor if they have either 2-3 members, or 7-10 members. This is likely a symptom of overfitting on sample data, rather than being reflective of the instability of the data generating process. This sampling variability is often the result of allowing highly flexible specifications, and something we will need to consider carefully when applying ML techniques.

The tenth order polynomial specification produces an even more jagged responsse surface with large areas of the conditioning set for which predicted income values fall outside of the observable income range.

```{r, echo=FALSE,warning=FALSE}
my.colors = colorRampPalette(c("#a50026", "red","orange","yellow","white","#4575b4", "#313695","black"))
grid.graph=data.frame(members=rep(seq(1,max(hhdata$members),0.1),each=((max(hhdata$rooms)-1)*10+1)),rooms=rep(seq(1,max(hhdata$rooms),0.1),((max(hhdata$members)-1)*10+1)))
h <-ggplot(grid.graph,aes(members, rooms))

lm.fit3=lm(income~poly(rooms,5)*poly(members,5),data=hhdata)
ols3.hat = lm.fit3$fitted.values
lm.fit4=lm(income~poly(rooms,10)*poly(members,10),data=hhdata)
ols4.hat = lm.fit3$fitted.values

ols1.hat.grid <- predict(lm.fit1, grid.graph)
ols2.hat.grid <- predict(lm.fit2, grid.graph)
ols3.hat.grid <- predict(lm.fit3, grid.graph)
ols4.hat.grid <- predict(lm.fit4, grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig1 = h + geom_point(aes(color = ols1.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("Linear")
fig2 = h + geom_point(aes(color = ols2.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Quadratic")
fig3 = h + geom_point(aes(color = ols3.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Order 5 polynomial")
fig4 = h + geom_point(aes(color = ols4.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Order 10 polynomial")

multiplot(fig1, fig3, fig2, fig4, cols=2)

```

## 3.1 In-sample fit

Econometricians will recognise the tradeoff between bias and sampling variability in their modelling decisions: moving from a linear to a quadratic specification makes sense if it substantially improves the model fit, whereas a polynomial of order 10 seems "too flexible" to model the kind of smooth trends we expect to see. The final decision regarding the specification is often made in a haphazardous way, starting with the simplest specification and then checking whether specific interaction effects or non-linear terms improve model fit. But the arbitrariness of this approach makes it vulnerable to misspecification, either because we do not explore all possible specifications or, more sinisterly, because we selectively present the results from specifications that support our preferred hypothesis. 

So how can we use the data to help us select the optimal specicificaiton? We start by using some goodness-of-fit measures for in-sample performance. The R-squared is one popular metric, but only measures how well the model fits the sample data. More flexible models will always produce higher R-squared values, without giving any warning of overfit. The adjusted R-squared penalises models with many parameters, so is more promising. Similarly, the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) both do something similar, and have the advantage of being more directly motivated by statistical theory.

We therefore proceed to estimate 100 models in which we combine polynomials of the number of rooms of orders 1 to 10 with polynomials of number of household members of orders 1 to 10. For each model we calculate the R-squared, the adjusted R-squared, the AIC and the BIC and then check which specification performs best for each of these metrics. The tables below report these calculated measures with the polynomial order for rooms varying across rows, and members varying across columns. 

```{r, results='asis',warning=FALSE}

R.squared=matrix(,nrow=maxvars,ncol=maxvars)
Adj.R.squared=matrix(,nrow=maxvars,ncol=maxvars)
AIC=matrix(,nrow=maxvars,ncol=maxvars)
BIC=matrix(,nrow=maxvars,ncol=maxvars)

for (rooms.i in (1:maxvars)) {
  for (members.i in (1:maxvars)) {
    R.squared[rooms.i,members.i]=summary(lm(income~poly(rooms,rooms.i)*poly(members,members.i),data=hhdata))$r.squared
    Adj.R.squared[rooms.i,members.i]=summary(lm(income~poly(rooms,rooms.i)*poly(members,members.i),data=hhdata))$adj.r.squared
    AIC[rooms.i,members.i]=AIC(lm(income~poly(rooms,rooms.i)*poly(members,members.i),data=hhdata),k=2)
    BIC[rooms.i,members.i]=AIC(lm(income~poly(rooms,rooms.i)*poly(members,members.i),data=hhdata),k=log(dim(hhdata)[1]))
  }
}
dimnames(R.squared) = list(rooms=1:maxvars, members=1:maxvars)
dimnames(Adj.R.squared) = list(rooms=1:maxvars, members=1:maxvars)
dimnames(AIC) = list(rooms=1:maxvars, members=1:maxvars)
dimnames(BIC) = list(rooms=1:maxvars, members=1:maxvars)
#stargazer(R.squared, type='html', title="R-squared",rownames=T, column.sep.width = "5pt",colnames=T,digits=4,initial.zero=T,no.space=F)
kable(R.squared,row.names=T,col.names=1:maxvars,digits=4,caption="R-squared")
paste("Maximum R-squared is achieved at", which(R.squared==max(R.squared),arr.ind=TRUE)[1], "rooms and",which(R.squared==max(R.squared),arr.ind=TRUE)[2], "members.")
kable(Adj.R.squared,row.names=T,col.names=1:maxvars,digits=4,caption="Adjusted R-squared")
paste("Maximum adjusted R-squared is achieved at", which(Adj.R.squared==max(Adj.R.squared),arr.ind=TRUE)[1], "rooms and", which(Adj.R.squared==max(Adj.R.squared),arr.ind=TRUE)[2], "members.")
kable(AIC,row.names=T,col.names=1:maxvars,digits=4,caption="AIC")
paste("Minimum AIC is achieved at", which(AIC==min(AIC),arr.ind=TRUE)[1], "rooms and", which(AIC==min(AIC),arr.ind=TRUE)[2], "members.")
kable(BIC,row.names=T,col.names=1:maxvars,digits=4,caption="BIC")
paste("Minimum BIC is achieved at", which(BIC==min(BIC),arr.ind=TRUE)[1], "rooms and", which(BIC==min(BIC),arr.ind=TRUE)[2], "members.")

```

Not surprisingly, the in-sample R-squared is maximised with the most flexible specificaition: polynomials of order 10. However, the goodness-of-fit measures that penalise model complexity choose more conservative specifications. The adjusted R-sqaured is marginally more conservative, with optimal polynomials of orders 9, whereas the AIC and BIC suggest much more parsimonious specifications of orders 4 and 3 polynomials respectively. 


## 3.2 Out-of-sample model fit

The penalised goodness-of-fit measures attempt to account for the inclination of complex models to overfit in-sample. A more straightforward way to achieve the same objective is to estimate the model on a training sample and to evaluate how well it performs on a testing sample. We therefore proceed by partioning the full data into a training and a testing sample (consisting of 75% and 25% of the full sample, repsectively). Each of the considered specifications is then estimated on the training sample and the estimates used to obtain predicted values for the testing sample. The root mean squared error (RMSE) is then calculated by comparing the observed and predicted household incomes in the tesing sample. The table below compares the RMSE for the different specifications. The out-of-sample model fit is optimised by a specification with a 4th order polynomial in rooms and a 3rd order polynomial in members. This is quite similar to what was recommended by the AIC or BIC in-sample measures.

```{r, results='asis', warning=FALSE}

set.seed(825)
inTraining <- createDataPartition(hhdata$income, p = .75, list = FALSE)
training <- hhdata[inTraining,1:3]
testing <- hhdata[-inTraining,1:3]
fitControl <- trainControl(method = "repeatedcv", number = 5,  repeats = 5)

RMSE=matrix(,nrow=maxvars,ncol=maxvars)
for (rooms.i in (1:maxvars)) {
  for (members.i in (1:maxvars)) {
    j <- bquote(income~poly(rooms,.(rooms.i))*poly(members,.(members.i)))
    LinearRegressor <- train(as.formula(j), data=training, method = "lm", trControl = fitControl)
    RMSE[rooms.i,members.i] <- LinearRegressor$results$RMSE
  }
}

dimnames(R.squared) = list(rooms=1:maxvars, members=1:maxvars)
dimnames(RMSE) = list(rooms=1:maxvars, members=1:maxvars)
kable(RMSE,row.names=T,col.names=1:maxvars,digits=4,caption="RMSE")
paste("Minimum RMSE is achieved at", which(RMSE==min(RMSE),arr.ind=TRUE)[1], "rooms and", which(RMSE==min(RMSE),arr.ind=TRUE)[2], "members.")

poly.fit.best=lm(income~poly(rooms,4)*poly(members,3),data=hhdata)
poly.best.grid <- predict(poly.fit.best, grid.graph)
best.poly = h + geom_point(aes(color = poly.best.grid)) + scale_colour_gradientn(colours = my.colors(20), 
                                                                    values = rescale(c(2,5.5,6,6.5,7,8,9,12)),limits=c(2, 12))  + ggtitle("OLS (Polynomial)") 

```

### Linear regressions with Fourier transforms

Although polynomial functions offer a useful way of extending the flexibility of linear models, they are known to produce predicted values that are highly sensitive to small adjustments in coefficient values. Tweaking the fit in high density data areas can cause highly variable and implausible predictions in low density areas. Other basis functions, like a series of Fourier transforms, are often preferred for this reason. We estimate a linear regression with Fourier transforms of orders 1, 2, 4 and 6 for each of the predictors and plot the predicted incomes below. 

```{r, results='asis', echo=FALSE,warning=FALSE}

series.estimation = function(rooms.n,members.n,train) {
  members.series.varlist = paste("cos((members)*", 1:members.n,"*pi/15)",sep="")
  rooms.series.varlist = paste("cos((rooms)*", 1:rooms.n,"*pi/15)",sep="")
  fmla=as.formula(paste("income ~ (",paste(members.series.varlist, collapse= "+"),")*(",paste(rooms.series.varlist, collapse= "+"),")"))
  lm(fmla,subset=train)
}

series.estimation.fmla = function(rooms.n,members.n) {
  members.series.varlist = paste("cos((members)*", 1:members.n,"*3.141593/15)",sep="")
  rooms.series.varlist = paste("cos((rooms)*", 1:rooms.n,"*3.141593/15)",sep="")
  fmla=as.formula(paste("income ~ (",paste(members.series.varlist, collapse= "+"),")*(",paste(rooms.series.varlist, collapse= "+"),")"))
}

series.fit1=series.estimation(1,1,NULL)
series.fit1.grid <- predict(series.fit1, grid.graph)
series.fit2=series.estimation(2,2,NULL)
series.fit2.grid <- predict(series.fit2, grid.graph)
series.fit4=series.estimation(4,4,NULL)
series.fit4.grid <- predict(series.fit2, grid.graph)
series.fit6=series.estimation(6,6,NULL)
series.fit6.grid <- predict(series.fit6, grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig1 = h + geom_point(aes(color = series.fit1.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("Fourier series of order 1")
fig2 = h + geom_point(aes(color = series.fit2.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Fourier series of order 2")
fig3 = h + geom_point(aes(color = series.fit4.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Fourier series of order 4")
fig4 = h + geom_point(aes(color = series.fit6.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Fourier series of order 6")

multiplot(fig1, fig3, fig2, fig4, cols=2)
```

Again, we observe the familiar pattern of an overly smooth response surface for models of low complexity, whereas models of high complexity produce an implausibly jagged surface with several local modes. The paramters that determine the model complexity - in this case the order for the Fourier transforms - can be tuned by comparing the out-of-sample performance of various models. The table below reports the RMSE values for Fourier transforms of orders 1 to 10 for rooms (rows) and members (columns).

```{r, results='asis', echo=FALSE,warning=FALSE}
set.seed(825)
inTraining <- createDataPartition(hhdata$income, p = .75, list = FALSE)
training <- hhdata[inTraining,1:3]
testing <- hhdata[-inTraining,1:3]
fitControl <- trainControl(method = "repeatedcv", number = 5,  repeats = 5)

RMSE=matrix(,nrow=maxvars,ncol=maxvars)
for (rooms.i in (1:maxvars)) {
  for (members.i in (1:maxvars)) {
    LinearRegressor <- train(series.estimation.fmla(rooms.i,members.i), data=training, method = "lm", trControl = fitControl)
    RMSE[rooms.i,members.i] <- LinearRegressor$results$RMSE
  }
}

dimnames(RMSE) = list(rooms=1:maxvars, members=1:maxvars)
kable(RMSE,row.names=T,col.names=1:maxvars,digits=4,caption="RMSE")
paste("Minimum RMSE is achieved at", which(RMSE==min(RMSE),arr.ind=TRUE)[1], "rooms and", which(RMSE==min(RMSE),arr.ind=TRUE)[2], "members.")

fourier.fit.best=series.estimation(2,7,NULL)
series.best.grid <- predict(fourier.fit.best, grid.graph)
best.series = h + geom_point(aes(color = series.best.grid)) + scale_colour_gradientn(colours = my.colors(20), 
                                                                    values = rescale(c(2,5.5,6,6.5,7,8,9,12)),limits=c(2, 12))  + ggtitle("OLS (Fourier)")
```

We see that out-of-sample prediction is most reliable when rooms is included as a Fourier transform of order 2 and members as a Fourier transform of order 7. We can use these tuned parameter values to estimate the final model on the entire sample.

### Regression trees

We could also model income using a regression tree. The rpart function in R requires specifying a complexity parameter (cp) which represents the smallest improvement in model fit required to enact a split. We estimate regression trees using 4 different cp values and graph the predicted values. 

```{r, echo=FALSE,warning=FALSE}

tree.fit1 =rpart(income~.,data=hhdata,control=rpart.control(minsplit=30, cp=0.01))
tree.fit2 =rpart(income~.,data=hhdata,control=rpart.control(minsplit=30, cp=0.005))
tree.fit3 =rpart(income~.,data=hhdata,control=rpart.control(minsplit=30, cp=0.0005))
tree.fit4 =rpart(income~.,data=hhdata,control=rpart.control(minsplit=30, cp=0.00001))

tree1.hat.grid=predict(tree.fit1, grid.graph)
tree2.hat.grid=predict(tree.fit2,grid.graph)
tree3.hat.grid=predict(tree.fit3,grid.graph)
tree4.hat.grid=predict(tree.fit4,grid.graph)

v = rescale(c(2,5.5,6,6.5,7,8,9,12))
l = c(2, 12)
fig1 = h + geom_point(aes(color = tree1.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  + ggtitle("Cp = 0.01")
fig2 = h + geom_point(aes(color = tree2.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Cp = 0.005")
fig3 = h + geom_point(aes(color = tree3.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Cp = 0.0005")
fig4 = h + geom_point(aes(color = tree4.hat.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l) + ggtitle("Cp = 0.000001")

multiplot(fig1, fig3, fig2, fig4, cols=2)

```

The regression tree complexity paramter can be tuned in the same way as before. We find that a cp value of 0.0002 produces the lowest test sample RMSE of 0.9906857.

```{r, echo=FALSE,warning=FALSE}

cv.eval = function(cp) {
  RMSE=rep(0,5)
  tree1.fit =rpart(income~.,data=hhdata[-sample1,],control=rpart.control(minsplit=30, cp=cp))
  tree2.fit =rpart(income~.,data=hhdata[-sample2,],control=rpart.control(minsplit=30, cp=cp))
  tree3.fit =rpart(income~.,data=hhdata[-sample3,],control=rpart.control(minsplit=30, cp=cp))
  tree4.fit =rpart(income~.,data=hhdata[-sample4,],control=rpart.control(minsplit=30, cp=cp))
  tree5.fit =rpart(income~.,data=hhdata[-sample5,],control=rpart.control(minsplit=30, cp=cp))
  RMSE[1] = sqrt(mean((income[sample1]-predict(tree1.fit,hhdata[sample1,]))^2))
  RMSE[2] = sqrt(mean((income[sample2]-predict(tree2.fit,hhdata[sample2,]))^2))
  RMSE[3] = sqrt(mean((income[sample3]-predict(tree3.fit,hhdata[sample3,]))^2))
  RMSE[4] = sqrt(mean((income[sample4]-predict(tree4.fit,hhdata[sample4,]))^2))
  RMSE[5] = sqrt(mean((income[sample5]-predict(tree5.fit,hhdata[sample5,]))^2))
  mean(RMSE)
}

cp = seq(from=2,to=5, by = 0.25)
cv.error.13= rep(0 ,13)
for (i in 1:13) {
  cv.error.13[i]=cv.eval(10^(-cp[i]))
}
#cp[which.min(cv.error.13)]

cp = seq(from = 3, to = 4, by =0.05)
cv.error.21= rep(0 ,21)
for (i in 1:21) {
  cv.error.21[i]=cv.eval(10^(-cp[i]))
}
#cp[which.min(cv.error.21)]

tree.fit.best =rpart(income~.,data=hhdata,control=rpart.control(minsplit=30, cp=10^(-3.7)))
tree.best.grid=predict(tree.fit.best, grid.graph)

best.tree = h + geom_point(aes(color = tree.best.grid)) + scale_colour_gradientn(colours = my.colors(20), values = v,limits=l)  +   
ggtitle("Tree")

```

# 4. Comparison of different models

Finally, we compare the income predictions for the tuned versions of all the models considered above: linear regressions with polynomial and Fourer transform basis function expansions, local linear regressions and regression trees. 

```{r, echo=FALSE, echo=FALSE,warning=FALSE}

multiplot(best.poly, best.knn, best.series, best.tree, cols=2)
```

Although attributes of the response surfaces are quite different (e.g. rectangular for regression trees, smooth for regressions) and the income predictions in the low density areas are quite different, all four models produce similar predicted incomes in the high density regions. In fact, a correlation matrix for the predicted values reveal that the pairwise correlation for any two of these models is never less than 0.976.

```{r, echo=FALSE, echo=FALSE,warning=FALSE}

best.polynomial=predict(poly.fit.best,hhdata)
best.fourier=predict(fourier.fit.best,hhdata)
best.loclinear=predict(knn.fit.best,hhdata)
best.tree=predict(tree.fit.best,hhdata)

d=data.frame(best.polynomial,best.fourier,best.loclinear,best.tree)
cor(d)

```
